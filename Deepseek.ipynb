{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.19","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9966459e-0a81-43fc-a109-6a7dde3251a9","cell_type":"code","source":"from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom dotenv import load_dotenv\nimport os\n\n# Load .env file\nload_dotenv()\n\n# Now access variables\napi_key = os.getenv('API_KEY')\n\nllm = ChatOpenAI(\n    model = \"deepseek-chat\",\n    api_key=api_key,\n    base_url=\"https://api.deepseek.com\",\n    temperature=0.7\n)\n\nprompt = ChatPromptTemplate.from_template(\"you are a {role},please anwser :{question}\")\n\nchain = prompt | llm\n\nresponse = chain.invoke({\"role\":\"AI expert\",\"question\":\"can Langchain integrate with Deepseek?\"})\n\nprint(response.content)\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Yes, **LangChain can integrate with DeepSeek**! ðŸš€\n\n## Current Integration Status\n\n**DeepSeek is officially supported in LangChain** through their chat models interface. You can use DeepSeek's models (like DeepSeek-Chat, DeepSeek-Coder) directly within LangChain applications.\n\n## How to Integrate\n\n### 1. **Install Required Packages**\n```bash\npip install langchain langchain-community\n```\n\n### 2. **Basic Usage Example**\n```python\nfrom langchain_community.chat_models import ChatDeepSeek\nfrom langchain.schema import HumanMessage\n\n# Initialize DeepSeek chat model\nchat = ChatDeepSeek(\n    model=\"deepseek-chat\",  # or \"deepseek-coder\"\n    api_key=\"your-deepseek-api-key\",\n    temperature=0.7\n)\n\n# Create a message\nmessages = [HumanMessage(content=\"Hello, how are you?\")]\n\n# Get response\nresponse = chat.invoke(messages)\nprint(response.content)\n```\n\n### 3. **With LangChain Chains**\n```python\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import ChatPromptTemplate\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful AI assistant\"),\n    (\"human\", \"{question}\")\n])\n\nchain = LLMChain(llm=chat, prompt=prompt)\nresult = chain.run(question=\"Explain quantum computing in simple terms\")\n```\n\n## Key Features in LangChain Integration\n\nâœ… **Chat Interface** - Full chat completion support  \nâœ… **Streaming** - Real-time response streaming  \nâœ… **Tool Calling** - Function calling capabilities  \nâœ… **Async Support** - Asynchronous operations  \nâœ… **Embedding Models** - Vector embedding support (if available)  \n\n## API Requirements\n\nYou'll need:\n- **DeepSeek API key** (from [platform.deepseek.com](https://platform.deepseek.com))\n- **Proper API endpoint configuration**\n\n## Use Cases with LangChain\n\n1. **Agents** - Create AI agents using DeepSeek as the reasoning engine\n2. **RAG Systems** - Build retrieval-augmented generation pipelines\n3. **Workflow Automation** - Complex multi-step reasoning chains\n4. **Tool Integration** - Connect with external tools and APIs\n\n## Limitations to Consider\n\nâš ï¸ **Rate Limits** - Be aware of DeepSeek's API rate limits  \nâš ï¸ **Model Availability** - Check which DeepSeek models are accessible via API  \nâš ï¸ **Cost** - Monitor API usage costs  \n\n## Best Practices\n\n1. **Use environment variables** for API keys\n2. **Implement error handling** for API failures\n3. **Cache responses** when appropriate\n4. **Monitor token usage** for cost optimization\n\nThe integration is quite seamless and allows you to leverage DeepSeek's strong reasoning capabilities within LangChain's powerful framework for building complex AI applications! ðŸŽ¯\n\nNeed help with a specific implementation?\n"}],"execution_count":1},{"id":"cfb8519f-cca5-4e4a-bbca-2cfe433e37fd","cell_type":"code","source":"response = chain.invoke({\"role\":\"Python expert\",\"question\":\"Use environment variables?\"})\n\nprint(response.content)\n\n","metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":"Yes, using environment variables in Python is a best practice for managing configuration, secrets, and environment-specific settings. Here's a comprehensive guide:\n\n## **Why Use Environment Variables?**\n- Keep secrets out of code (API keys, passwords)\n- Different configurations per environment (dev/staging/prod)\n- Easy configuration changes without code modifications\n- Follows the 12-factor app methodology\n\n## **Basic Usage**\n\n### **1. Reading Environment Variables**\n```python\nimport os\n\n# Get an environment variable\napi_key = os.getenv('API_KEY')\n\n# With default value\napi_key = os.getenv('API_KEY', 'default_value')\n\n# Alternative (raises KeyError if not found)\napi_key = os.environ['API_KEY']\n```\n\n### **2. Setting Environment Variables**\n\n**In terminal/shell:**\n```bash\n# Linux/Mac\nexport DATABASE_URL=\"postgresql://user:pass@localhost/db\"\n\n# Windows (Command Prompt)\nset DATABASE_URL=postgresql://user:pass@localhost/db\n\n# Windows (PowerShell)\n$env:DATABASE_URL=\"postgresql://user:pass@localhost/db\"\n```\n\n**In Python:**\n```python\nimport os\nos.environ['DATABASE_URL'] = 'postgresql://user:pass@localhost/db'\n```\n\n## **Best Practices**\n\n### **1. Use a `.env` File (Recommended)**\nCreate a `.env` file:\n```env\n# .env\nDATABASE_URL=postgresql://user:pass@localhost/db\nAPI_KEY=your_api_key_here\nDEBUG=True\n```\n\n**Install python-dotenv:**\n```bash\npip install python-dotenv\n```\n\n**Load in your Python code:**\n```python\nfrom dotenv import load_dotenv\nimport os\n\n# Load .env file\nload_dotenv()\n\n# Now access variables\ndb_url = os.getenv('DATABASE_URL')\napi_key = os.getenv('API_KEY')\n```\n\n### **2. Create a Configuration Module**\n```python\n# config.py\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass Config:\n    DATABASE_URL = os.getenv('DATABASE_URL')\n    API_KEY = os.getenv('API_KEY')\n    DEBUG = os.getenv('DEBUG', 'False').lower() == 'true'\n    SECRET_KEY = os.getenv('SECRET_KEY', 'dev-secret-key')\n    \n    @classmethod\n    def validate(cls):\n        required = ['DATABASE_URL', 'API_KEY']\n        for var in required:\n            if not getattr(cls, var):\n                raise ValueError(f\"Missing required env var: {var}\")\n```\n\n### **3. Type Conversion Helpers**\n```python\ndef get_env_bool(key: str, default: bool = False) -> bool:\n    value = os.getenv(key, str(default)).lower()\n    return value in ('true', '1', 'yes', 'on')\n\ndef get_env_int(key: str, default: int = 0) -> int:\n    try:\n        return int(os.getenv(key, default))\n    except ValueError:\n        return default\n\ndef get_env_list(key: str, separator: str = ',', default: list = None) -> list:\n    if default is None:\n        default = []\n    value = os.getenv(key)\n    return value.split(separator) if value else default\n```\n\n## **Advanced Patterns**\n\n### **1. Environment-Specific Configuration**\n```python\n# config.py\nimport os\nfrom enum import Enum\n\nclass Environment(Enum):\n    DEVELOPMENT = \"development\"\n    STAGING = \"staging\"\n    PRODUCTION = \"production\"\n\nclass Config:\n    ENV = Environment(os.getenv('ENVIRONMENT', 'development'))\n    \n    if ENV == Environment.DEVELOPMENT:\n        DEBUG = True\n        DATABASE_URL = os.getenv('DEV_DB_URL')\n    elif ENV == Environment.PRODUCTION:\n        DEBUG = False\n        DATABASE_URL = os.getenv('PROD_DB_URL')\n```\n\n### **2. Using Pydantic (Modern Approach)**\n```python\n# pip install pydantic pydantic-settings\nfrom pydantic_settings import BaseSettings\nfrom typing import Optional\n\nclass Settings(BaseSettings):\n    database_url: str\n    api_key: str\n    debug: bool = False\n    port: int = 8000\n    \n    class Config:\n        env_file = \".env\"\n        env_file_encoding = 'utf-8'\n\n# Automatically loads from .env and environment\nsettings = Settings()\nprint(settings.database_url)\n```\n\n### **3. Docker Integration**\n```dockerfile\n# Dockerfile\nFROM python:3.11\n\n# Set environment variables\nENV PYTHONUNBUFFERED=1\nENV APP_ENV=production\n\n# Or pass at runtime\n# docker run -e DATABASE_URL=... -e API_KEY=... myapp\n```\n\n## **Security Considerations**\n\n1. **Never commit `.env` files** - Add to `.gitignore`\n2. **Use different `.env` files** per environment\n3. **Validate required variables** at startup\n4. **Use secret management services** for production:\n   - AWS Secrets Manager\n   - HashiCorp Vault\n   - Azure Key Vault\n   - Google Secret Manager\n\n## **Example Project Structure**\n```\nmyproject/\nâ”œâ”€â”€ .env.example          # Template with placeholder values\nâ”œâ”€â”€ .env                  # Local development (gitignored)\nâ”œâ”€â”€ .env.production       # Production (gitignored)\nâ”œâ”€â”€ config/\nâ”‚   â””â”€â”€ settings.py       # Configuration module\nâ”œâ”€â”€ requirements.txt\nâ””â”€â”€ app.py\n```\n\n## **Quick Start Template**\n```python\n# app.py\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Configuration\nconfig = {\n    'database': os.getenv('DATABASE_URL'),\n    'api_key': os.getenv('API_KEY'),\n    'debug': os.getenv('DEBUG', 'False').lower() == 'true',\n}\n\n# Validate\nif not config['api_key']:\n    raise ValueError(\"API_KEY is required\")\n\nprint(f\"Running in {'debug' if config['debug'] else 'production'} mode\")\n```\n\nThis approach ensures your application is secure, configurable, and follows modern development practices.\n"}],"execution_count":2},{"id":"1f2454a7-4135-41b2-9c4c-0f0f3fbd4d8f","cell_type":"code","source":"response = chain.invoke({\"role\":\"AI expert\",\"question\":\"how to use langchain RAG Systems ?\"})\nprint(response.content)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"I'll explain how to use LangChain's RAG (Retrieval-Augmented Generation) system step by step.\n\n## **1. Core Components of RAG in LangChain**\n\n### **Basic Setup**\n```python\nfrom langchain.vectorstores import Chroma, FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\n```\n\n## **2. Step-by-Step Implementation**\n\n### **Step 1: Load Documents**\n```python\nfrom langchain.document_loaders import (\n    TextLoader, \n    PyPDFLoader, \n    WebBaseLoader,\n    DirectoryLoader\n)\n\n# Load from various sources\nloader = PyPDFLoader(\"document.pdf\")\n# or\nloader = WebBaseLoader([\"https://example.com\"])\n# or\nloader = DirectoryLoader(\"./docs\", glob=\"**/*.txt\")\n\ndocuments = loader.load()\n```\n\n### **Step 2: Split Documents into Chunks**\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len,\n)\n\nchunks = text_splitter.split_documents(documents)\n```\n\n### **Step 3: Create Vector Store**\n```python\n# Using OpenAI embeddings\nembeddings = OpenAIEmbeddings()\n\n# Option 1: Chroma (persistent)\nvectorstore = Chroma.from_documents(\n    documents=chunks,\n    embedding=embeddings,\n    persist_directory=\"./chroma_db\"\n)\n\n# Option 2: FAISS (in-memory)\nvectorstore = FAISS.from_documents(chunks, embeddings)\nvectorstore.save_local(\"faiss_index\")\n```\n\n### **Step 4: Create Retriever**\n```python\n# Basic retriever\nretriever = vectorstore.as_retriever(\n    search_type=\"similarity\",  # or \"mmr\" for diversity\n    search_kwargs={\"k\": 4}  # number of documents to retrieve\n)\n\n# Advanced retriever with filtering\nretriever = vectorstore.as_retriever(\n    search_kwargs={\n        \"k\": 6,\n        \"filter\": {\"category\": \"technical\"}  # metadata filtering\n    }\n)\n```\n\n### **Step 5: Create RAG Chain**\n```python\n# Option 1: Simple RetrievalQA\nqa_chain = RetrievalQA.from_chain_type(\n    llm=OpenAI(temperature=0),\n    chain_type=\"stuff\",  # \"map_reduce\", \"refine\", \"map_rerank\"\n    retriever=retriever,\n    return_source_documents=True\n)\n\n# Option 2: Conversational RAG\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    return_messages=True\n)\n\nconversational_chain = ConversationalRetrievalChain.from_llm(\n    llm=OpenAI(),\n    retriever=retriever,\n    memory=memory\n)\n```\n\n### **Step 6: Query the System**\n```python\n# Simple query\nresult = qa_chain({\"query\": \"What is machine learning?\"})\nprint(result[\"result\"])\nprint(result[\"source_documents\"])\n\n# Conversational query\nresult = conversational_chain({\"question\": \"Explain neural networks\"})\nfollow_up = conversational_chain({\"question\": \"How do they differ from CNNs?\"})\n```\n\n## **3. Advanced RAG Patterns**\n\n### **Hybrid Search (Dense + Sparse)**\n```python\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.vectorstores import FAISS\n\n# Create dense retriever\nvectorstore = FAISS.from_documents(chunks, embeddings)\ndense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n\n# Create sparse retriever\nbm25_retriever = BM25Retriever.from_documents(chunks)\nbm25_retriever.k = 3\n\n# Combine both\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, dense_retriever],\n    weights=[0.5, 0.5]\n)\n```\n\n### **Reranking for Better Results**\n```python\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\ncompressor = LLMChainExtractor.from_llm(OpenAI())\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=retriever\n)\n```\n\n### **Multi-Query Retriever**\n```python\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\n\nmulti_query_retriever = MultiQueryRetriever.from_llm(\n    retriever=vectorstore.as_retriever(),\n    llm=OpenAI()\n)\n```\n\n## **4. Complete Working Example**\n\n```python\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nimport os\n\n# Set API key\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\n# 1. Load document\nloader = TextLoader(\"knowledge_base.txt\")\ndocuments = loader.load()\n\n# 2. Split text\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\nchunks = text_splitter.split_documents(documents)\n\n# 3. Create embeddings and vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(\n    chunks, \n    embeddings,\n    persist_directory=\"./chroma_db\"\n)\n\n# 4. Create retriever\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n\n# 5. Create QA chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=OpenAI(temperature=0),\n    chain_type=\"stuff\",\n    retriever=retriever,\n    return_source_documents=True\n)\n\n# 6. Query\nquery = \"What are the key points in the document?\"\nresponse = qa_chain({\"query\": query})\nprint(f\"Answer: {response['result']}\")\nprint(f\"Sources: {[doc.metadata for doc in response['source_documents']]}\")\n```\n\n## **5. Best Practices & Tips**\n\n1. **Chunk Size**: Start with 500-1000 characters, adjust based on content\n2. **Overlap**: Use 10-20% overlap between chunks\n3. **Metadata**: Add metadata to chunks for better filtering\n4. **Evaluation**: Use `ragas` or `truelens` for RAG evaluation\n5. **Caching**: Implement caching for frequent queries\n6. **Hybrid Search**: Combine semantic and keyword search for better recall\n\n## **6. Common Variations**\n\n```python\n# Using different LLMs\nfrom langchain.chat_models import ChatOpenAI, ChatAnthropic\n\n# Using different vector stores\nfrom langchain.vectorstores import Pinecone, Weaviate, Qdrant\n\n# Parent document retriever (for hierarchical chunks)\nfrom langchain.retrievers import ParentDocumentRetriever\n```\n\n## **7. Monitoring & Evaluation**\n\n```python\n# Logging queries and responses\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Using LangSmith for tracing\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"your-langsmith-key\"\n```\n\nThis covers the essentials of implementing RAG with LangChain. The framework is highly modular, allowing you to swap components based on your specific needs.\n"}],"execution_count":3},{"id":"a954234e-584a-491e-93ad-18f31abeeca8","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
